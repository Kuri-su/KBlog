# Prof. Andrew Ng ML 2014 Course Summary Note

[TOC]

## L1 Introduction

// skip

## L2 Linear Regression with One Variable

* question category
  * Regression problem (回归问题)
  * Classification problem (分类问题)
* symbol introduction
  * $m$ : number of training examples
  * $x$ : "input" variable/features
  * $y$ : "output" variable | "target" variable
  * $(x,y)$ : single training example
  * $(x^{(i)},y^{(i)})$ : $i^{th}$ training example (i row example)
* **hypothesis** (假设函数) <u>in one variable linear regression</u>
  * $h_\theta(x)=\theta_0+\theta_1 x$
    * $\theta$ : model parameters (constant)
* linear regression (线性回归)
  * univariate/one variable linear regression (单一变量线性回归)
* **Cost functions** (损失函数/代价函数) <u>in one variable linear regression</u>
  * $\frac{1}{2m} \sum\limits_{i=1}^{m}(h_\theta(x^{(i)}),y^{(i)})^2 ,\ \ minimize$
    * A.K.A. Squared error cost function (平方误差函数)
  *  def $J(\theta_0,\theta_1)=\frac{1}{2m} \sum\limits_{i=1}^{m}(h_\theta(x^{(i)}),y^{(i)})^2$
  * want minimize $J(\theta_0,\theta_1)$
* **Gradient descent **

## L3 Linear Algebra Review

## L5 Octave/Matlab Tutorial

// skip

## L6

## L7



## L16

// unlearned

## L17

// unlearned

## L18 Application Example: Photo OCR

## L19 Conclusion

// skip

## ref

* https://www.coursera.org/learn/machine-learning
* https://scruel.gitee.io/ml-andrewng-notes/
* http://www.ai-start.com/ml2014/
* https://github.com/SrirajBehera/Machine-Learning-Andrew-Ng
* https://cs229.stanford.edu/lectures-spring2022/main_notes.pdf